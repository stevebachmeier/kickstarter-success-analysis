{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kickstarter Project Success Analysis\n",
    "Steve Bachmeier <br>\n",
    "2018-12-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "df_results = dill.load(open(\"df_results.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Synopsis\n",
    "Several machine learning models were trained on a cleaned training set (60% of the entire dataset) and tested on a cleaned test set (20% of the entire dataset) - the results of these models are shown in the table below. \n",
    "\n",
    "An optimized random forest model provides a decent **10-fold cross validation mean accuracy of 71.6% while also featuring a (qualitatively) quick run-time**. \n",
    "\n",
    "The specific random forest model used features 100 trees, Gini impurity criterion, the maximum number of features to consider for a split equal to the square root of all of the features, and the minimum number of samples required to be at a leaf node of 25 (ie *n_estimators*=100, *criterion*=\"gini\", *max_features*='sqrt', and *min_samples_leaf*=25)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>time_fit</th>\n",
       "      <th>time_predict</th>\n",
       "      <th>time_10_fold_CV</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>acc_10_fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.101767</td>\n",
       "      <td>0.036270</td>\n",
       "      <td>1.53579</td>\n",
       "      <td>0.623551</td>\n",
       "      <td>0.624725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>1.822522</td>\n",
       "      <td>0.009085</td>\n",
       "      <td>17.0696</td>\n",
       "      <td>0.694333</td>\n",
       "      <td>0.689984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>K Nearest Neighbors</td>\n",
       "      <td>7.340739</td>\n",
       "      <td>101.319590</td>\n",
       "      <td>344.234</td>\n",
       "      <td>0.682739</td>\n",
       "      <td>0.675807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SVM, Linear</td>\n",
       "      <td>954.375944</td>\n",
       "      <td>90.726687</td>\n",
       "      <td>None</td>\n",
       "      <td>0.677168</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVM, RBF</td>\n",
       "      <td>1091.777649</td>\n",
       "      <td>120.427602</td>\n",
       "      <td>None</td>\n",
       "      <td>0.682908</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.437127</td>\n",
       "      <td>0.014542</td>\n",
       "      <td>4.82765</td>\n",
       "      <td>0.664357</td>\n",
       "      <td>0.662544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Random Forest (10-fold)</td>\n",
       "      <td>1.016743</td>\n",
       "      <td>0.104582</td>\n",
       "      <td>10.463</td>\n",
       "      <td>0.683248</td>\n",
       "      <td>0.680784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PCA (n=2), Naive Bayes</td>\n",
       "      <td>0.024343</td>\n",
       "      <td>0.003595</td>\n",
       "      <td>0.284533</td>\n",
       "      <td>0.609355</td>\n",
       "      <td>0.608927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Random Forest (Optimized)</td>\n",
       "      <td>5.856856</td>\n",
       "      <td>0.470576</td>\n",
       "      <td>66.9746</td>\n",
       "      <td>0.718257</td>\n",
       "      <td>0.716067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Logistic Regression (Optimized)</td>\n",
       "      <td>6.454474</td>\n",
       "      <td>0.003956</td>\n",
       "      <td>68.8582</td>\n",
       "      <td>0.695662</td>\n",
       "      <td>0.691059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             model     time_fit  time_predict time_10_fold_CV  \\\n",
       "0                      Naive Bayes     0.101767      0.036270         1.53579   \n",
       "1              Logistic Regression     1.822522      0.009085         17.0696   \n",
       "2              K Nearest Neighbors     7.340739    101.319590         344.234   \n",
       "3                      SVM, Linear   954.375944     90.726687            None   \n",
       "4                         SVM, RBF  1091.777649    120.427602            None   \n",
       "5                    Decision Tree     0.437127      0.014542         4.82765   \n",
       "6          Random Forest (10-fold)     1.016743      0.104582          10.463   \n",
       "7           PCA (n=2), Naive Bayes     0.024343      0.003595        0.284533   \n",
       "8        Random Forest (Optimized)     5.856856      0.470576         66.9746   \n",
       "9  Logistic Regression (Optimized)     6.454474      0.003956         68.8582   \n",
       "\n",
       "   accuracy acc_10_fold  \n",
       "0  0.623551    0.624725  \n",
       "1  0.694333    0.689984  \n",
       "2  0.682739    0.675807  \n",
       "3  0.677168        None  \n",
       "4  0.682908        None  \n",
       "5  0.664357    0.662544  \n",
       "6  0.683248    0.680784  \n",
       "7  0.609355    0.608927  \n",
       "8  0.718257    0.716067  \n",
       "9  0.695662    0.691059  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General trends were not so easy to recognize with the exception of the influence of the variable *staff_pick*; *staff_pick* is the predictor most highy correlated with *launch_state*:\n",
    "* *staff_pick* - *launch_state* correlation: 25%\n",
    "* 53% of projects without *staff_pick* succeed\n",
    "* 89% of projects with *staff_pick* succeed\n",
    "\n",
    "From https://www.kickstarter.com/blog/how-to-get-featured-on-kickstarter, it appears as if projects are featured when they catch the eye of the Kickstarter staff via creativity, a nice and visually appealing site, etc. ie, they are **not** just picked due to them being funded well. **Projects that are featured on Kickstarter (ie *staff_pick* = 1) are more likely to be successfully funded.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Background\n",
    "Having spent six years as a mechanical engineer in the silicon valley where ideas are big but funding is small, I've always been intrigued by the concept of crowd-funding. As an end user/backer, however, we want to maximize the chances that the projects we back actually successfully launch. This project uses historical data from the popular project-launching website Kickstarter to look for trends and make predictions about whether a project is likely to be successfully funded or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data\n",
    "The raw Kickstarter data (the JSON file updated at 2018-10-18) was downloaded from: https://webrobots.io/kickstarter-datasets/. It is assumed that this data is accurate and no attempt was made to verify the web scraping tools used.\n",
    "\n",
    "Notes from raw data downloaded:\n",
    "\n",
    "* **From April 2015 we noticed that Kickstarter started limiting how many projects user can view in a single category. This limits the amount of historic projects we can get in a single scrape run. But recent and active projects are always included.**\n",
    "* **From December 2015 we modified the collection approach to go through all sub-categories instead of only top level categories. This yields more results in the datasets, but possible duplication where projects are listed in multiple categories. Also from December 2015 JSON file is in JSON streaming format. Read more about it here: https://en.wikipedia.org/wiki/JSON_Streaming**\n",
    "* **We receive many question about timestamp format used in this dataset. It is unix time. Google has a lot of information about it.**\n",
    "* **Files are compressed, size in area of 100mb. Uncompressed size around 600mb.**\n",
    "\n",
    "Note that no attempt was made for this project to ensure we have the entirety of the project history. Also, due to Github size constraints, the raw dataset is not uploaded to this repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Goal\n",
    "There are two potential goals of this project:\n",
    "\n",
    "1. Analyze the raw data obtained to look for any interesting trends.\n",
    "\n",
    "2. Build a prediction algorithm to try and predict whether future projects will successfully launch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Analyisis\n",
    "This section outlines the analysis completed. Refer to the [appendix](#Appendix) for relevant code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data preparation\n",
    "\n",
    "Refer to appendix [A1.1 Data preparation](#A1.1) for code.\n",
    "\n",
    "The downloaded dataset from https://webrobots.io/kickstarter-datasets/ came in JSON format; each of the 205,696 rows representing a project's details that were wrapped up in a serialized set of dictionaries and nested dictionaries. Unpacking this file was not trivial - the summary of the process is as follows:\n",
    "\n",
    "1. Open the file with utf8 encoding and load each line to a new object.\n",
    "\n",
    "2. The raw object includes four columns of dictionaries of which only the *data* column is relevant; extract that *data* column.\n",
    "\n",
    "3. Convert the json file to a Pandas dataframe.\n",
    "\n",
    "4. Unpack each dictionary with *json_normalize()*. Note that this does not unpack columns with NaN values.\n",
    "\n",
    "5. Unpack remaining dictionary columns (those with NaN values) manually by applying the Pandas *Series()* method.\n",
    "\n",
    "6. Concatenate the newly unpacked columns to the dataframe.\n",
    "\n",
    "7. Drop the original json columns from the dataframe.\n",
    "\n",
    "8. Split the dataframe into a working set (later to be the train and test set) and a validation set. We use a random 20% smapling of the entire raw dataset for the validatio set.\n",
    "\n",
    "```\n",
    "X, X_v, y, y_v = train_test_split(df_raw.drop(columns=['state']), \n",
    "                                  df_raw['state'], test_size=0.2, \n",
    "                                  random_state=101)\n",
    "```\n",
    "\n",
    "The working and validation sets now consist of 97 columns (where each columns is a different variable with one of them being the outcome)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Data cleaning\n",
    "\n",
    "Refer to appendix [A1.2 Data cleaning](#A1.2) for code.\n",
    "\n",
    "With the raw data now in a useable dataframe, we can clean it up for machine learning use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Clean up columns\n",
    "\n",
    "The first step is to drop clearly useless variables. This demands some amount of reasoning. For example, it is perhaps obvious that a project photo urls, creator avatar photos, and creator profile blurbs are not useful for using machine learning to make predictions. However, other variables may not be so obvious. For example, a creator's name could be used to identify the gender (which is not provided directly in the dataset) which in turn might shed some light on project success (note that for this analysis I did indeed drop the creator's name from the dataset).\n",
    "\n",
    "One interesting variable that took special consideration is *profile_state*. Digging into it showed that there are only two unique values for *profile_state*: 'active' and 'inactive'. Further, only 11.7% of the project profiles are labeled 'active'. It is assumed that projects go 'inactive' after a certain period of latency and so cannot be used in predicting project success (ie even a successfully funded project profile may go active after some amount of time past the deadling). I decided to drop *profile_state*.\n",
    "\n",
    "Another tricky one was *usd_type*. Frankly, I was unable to get a firm grasp on what exactly it is. There were many instances of a project country being labeled, say 'US' with it's currency being 'USD' but then *usd_type* being 'international'. Further, there were instances of empty values. Finally, the vast majority of *usd_type* is labeled international. I decided to drop it.\n",
    "\n",
    "Other unobvious variables that I deleted include: *name*, *blurb*, *loc_state* (too granular), *location_country* (largely redundant with *loc_country* but far more granular), *currency* (the pledges are all in USD), *currency_trailing_code*, and *state_changed_at*.\n",
    "\n",
    "Once the useless columns were dropped, I renamed *category_slug* to *category* and *state* to *launch_state*. \n",
    "\n",
    "Finally, I reordered the columns into a more intuitive order including putting *launch_state* first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Extract categories\n",
    "\n",
    "The *category* column (initially labeled *category_slug*) included primary categories and sub-categories in the format 'primary_category/sub_category', eg 'art/painting' and 'comics/webcomics'. I simplified the *category* variable by extracting the first word, eg 'art/painting' became 'art' and 'comics/webcomics' became 'comics'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 Drop duplicate rows\n",
    "There were a fair amount of duplicate rows which are easy to remove with ```df.drop_duplicates(inplace=True)```. However, even after this, the dataset included rows that were mostly duplicates with the exception of just a few column values. In order to keep the dataset tidy (where each row is a unique observation or, in this case, project), I had to remove any rows with duplicate project IDs. I decided to, in the case of duplicate IDs, keep those with the highest *pledged* value (assuming that this was input after the other rows and so is more accurate). This was accomplished with ```df = df.sort_values('pledged', ascending=False).drop_duplicates('id').sort_index()```. These two steps resulted in no duplicate ID values and so a tidy dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.4 Convert relevant values to datetime\n",
    "\n",
    "At this point, *deadline* and *launched_at* contained string values that are in the unix timestamp format. These variables were converted to datetime via\n",
    "\n",
    "```\n",
    "df['deadline'] = df['deadline'].apply(datetime.utcfromtimestamp)\n",
    "df['launched_at'] = df['launched_at'].apply(datetime.utcfromtimestamp)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.5 NA / Null / empty value imputation\n",
    "At this point the dataframe was completely clean of any NA, Null, or empty values. This was easily checked with\n",
    "\n",
    "```\n",
    "if df.isnull().sum().sum() != 0:\n",
    "    print('*** WARNING: There are null values ***')\n",
    "if df.isna().sum().sum() != 0:\n",
    "    print('*** WARNING: There are NA values ***')\n",
    "if (df=='').sum().sum() != 0:\n",
    "    print('*** WARNING: There are empty string (\\'\\') values ***')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.6 Clean up the outcome variabe *launch_state*\n",
    "\n",
    "There are five values for *launch_state* (previously *state*): 'failed', 'successful', 'canceled', 'live', and 'suspended'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/launch_state.jpeg\" alt=\"launch_state count plot\" style=\"height: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this project, it makes sense to keep only 'failed' and 'successful' projects (since those labeled 'canceled' and 'suspended' cannot be backed to begin with and those labeled 'live' are exactly the projects we are trying to predict). We thus query only *launch_state* valuse of 'failed' and 'successful' (and type None just for completeness).\n",
    "\n",
    "```\n",
    "df.query(\"launch_state == 'failed' | \"\n",
    "         \"launch_state == 'successful' | \"\n",
    "         \"launch_state == None\", inplace=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.7 Create dummy variables\n",
    "\n",
    "Of the remaining variables, *category* and *country* are categorical, ie they are labels rather than numbers. For the machine learning algorithm, I needed to convert these to dummy variables:\n",
    "\n",
    "```\n",
    "category = pd.get_dummies(df['category'], drop_first=True)\n",
    "country = pd.get_dummies(df['country'], drop_first=True)\n",
    "```\n",
    "\n",
    "Note that I did drop the first dummy variable to ensure the correct degrees of freedom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.8 Convert remaining string variables to integers\n",
    "\n",
    "There were still several variables with categorical binary values that could be converted to binary integers.\n",
    "\n",
    "* *launch_state*: ['failed', 'successful'] should be [0, 1]\n",
    "* *staff_pick*: [False, True] should be [0, 1]\n",
    "* *spotlight*: [False, True] should be [0, 1]\n",
    "\n",
    "\n",
    "I created a dictionaries to define what the string should be converted to and then mapped it to the relevant variables.\n",
    "\n",
    "```\n",
    "d_launch_state = dict(zip(['failed','successful'], range(0,2)))\n",
    "launch_state = df['launch_state'].map(d_launch_state)\n",
    "\n",
    "d_staff_pick = dict(zip([False,True], range(0,2)))\n",
    "staff_pick = df['staff_pick'].map(d_staff_pick)\n",
    "  \n",
    "d_spotlight = dict(zip([False,True], range(0,2)))\n",
    "spotlight = df['spotlight'].map(d_spotlight)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Variable reduction\n",
    "At this point we have a tidy dataframe with 141,447 rows and 46 variables (most of which are dummy *country* and *category* variables). We can now look into paring down the number of variables to help reduce over-fitting of the machine learning algorithm(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 Zero variance\n",
    "We start by checking for zero variance variables, ie those variables that do not change at all throughout the entire dataset. Note that we've already removed some of these during the data cleaning phase, but it's still a good check.\n",
    "\n",
    "```\n",
    "sel = VarianceThreshold(threshold=0.0)\n",
    "sel.fit_transform(X=df.drop(columns=info_variables)).shape[1] - df.drop(columns=info_variables).shape[1]\n",
    "```\n",
    "\n",
    "There were no zero variance variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 Near-zero variance\n",
    "It was decided not to search for or remove near-zero variance variables. One reason is that many of the dummy variables will certainly have variances near zero, eg a particularly small *country* may only appear a handfull of times in the entire data set and so the vast majority of values will be 0. Further, there is evidence that near-zero variance variables can still have a significant impace on the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.3 Variable - outcome correlation\n",
    "Variables with a very high correlation to the outcome we are trying to predict should be given extra consideration and possibly dropped. For this analysis, I use a threshold correlation of 0.5 - a single variable is found as shown in the plot below. Note that the dashed red line is the threshold value.\n",
    "\n",
    "<img src=\"images/variable_outcome_correlation.jpeg\" alt=\"variable-outcome correlation plot\" style=\"height: 400px;\"/>\n",
    "\n",
    "It turns out that this single variable of interest is *spotlight* which, as shown in the plot, has a perfect correlation of 1 with *launch_state*. In other words, it's a perfect predictor (which obviously seems suspicious). From https://techcrunch.com/2015/03/25/kickstarter-spotlight/, we see that spotlight happens for successfully funded projects and acts as a way to update the project timeline. It clearly does nothing in helping predict funding success; I dropped it.\n",
    "\n",
    "The next highest correlation is *staff_pick* at 0.25, well under the threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.4 Variable-variable correlation\n",
    "Next we consider multicollinearity, ie where a variable can be predicted by other variables. One way to battle this is by ensuring all variables have a variable-variable corration beneath some threshold. For this analysis, we assume this threshold is 0.5.\n",
    "\n",
    "The trick here is to create an upper correlation matrix with the ones diagonal removed, unstack it, sort the values in descending order, and filter by all correlation values greater than the threshold.\n",
    "\n",
    "```\n",
    "corMat_upper = corMat.where(np.triu(np.ones(corMat.shape), k=1).astype(np.bool))\n",
    "corMat_upper.unstack().sort_values(kind='quicksort')[corMat_upper.unstack().sort_values(kind='quicksort') > .5]\n",
    "```\n",
    "\n",
    "The result is that a single variable pair has a correlation larger than 0.5:\n",
    "\n",
    "<img src=\"images/variable_variable_correlation.jpeg\" alt=\"variable-variable correlation plot\" style=\"height: 400px;\"/>\n",
    "\n",
    "The variable pair in question is *US* - *GB* and has a correlation of 0.599. It does not make sense to drop a country just because it correlates with another country and so we keep both *US* and *GB* dummy variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Exploratory data analysis\n",
    "It is always a good idea to do at least a bit of exploratory data analysis before diving into the machine learning aspect of a project. Creating visualizations can uncover interesting trends and also help guide further analysis.\n",
    "\n",
    "A pairplot of all of the non-dummy variables is shown below. Unfortunately, there does not seem to be good separation between 'successful' and 'failed' projects for any single variable.\n",
    "\n",
    "<img src=\"images/pair_plot.jpeg\" alt=\"pair plot\" style=\"height: 400px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X Next steps\n",
    "\n",
    "Some recommendations to improve this analysis include:\n",
    "\n",
    "* Complete an analysis to determine the statistical influence of the 'staff_pick' variable, ie while it appears as if staff_pick = 1 results in a higher chance that a project is successfully funded, is this statistically accurate?\n",
    "* Re-run the analysis without the country dummy variables.\n",
    "* Re-run the analysis without the category dummy variables.\n",
    "* Code the finished script so that it can accept raw data without every column. Specifically, a small enough amount of raw data may not include all of the required countries or categories currenty required to fit the model.\n",
    "* Analyze the effect of including sub-categories in the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Appendix'></a>\n",
    "\n",
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='A1'></a>\n",
    "\n",
    "# A1 Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='A1.1'></a>\n",
    "\n",
    "## A1.1 Data preparation (as of 2018-12-12)\n",
    "\n",
    "**f_dataImport.py**\n",
    "\n",
    "```\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Nov 27 14:06:31 2018\n",
    "\n",
    "@author: steve\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#==============================================================================\n",
    "#\n",
    "# IMPORT LIBRARIES\n",
    "#\n",
    "#==============================================================================\n",
    "import pandas as pd\n",
    "import os\n",
    "from pandas.io.json import json_normalize \n",
    "import json\n",
    "\n",
    "#==============================================================================\n",
    "#\n",
    "# FUNCTION - RAW DATA IMPORT\n",
    "#\n",
    "#==============================================================================\n",
    "\n",
    "def dataImport():\n",
    "    '''\n",
    "    This function imports the raw json data downloaded from \n",
    "    https://webrobots.io/kickstarter-datasets/ and extracts the dictionaries\n",
    "    into a usable dataframe format.\n",
    "    \n",
    "    OUTPUTS:\n",
    "        * 'df_raw.csv': raw data dataframe\n",
    "    '''\n",
    "    \n",
    "    #-----------------------------------------\n",
    "    # READ IN RAW DATA\n",
    "    \n",
    "    print('\\n')\n",
    "    print('***')\n",
    "    print('NOTE:')\n",
    "    print('The input file must be JSON with the same format as those'\n",
    "          'downloaded from https://webrobots.io/kickstarter-datasets/')\n",
    "    print('***')\n",
    "        \n",
    "    while True:\n",
    "    \n",
    "        print('\\n')\n",
    "        new_data = str(input('Input the new data JSON filepath you want to predict: '))\n",
    "        \n",
    "        if not os.path.exists(new_data):\n",
    "            print('\\n')\n",
    "            print('The filepath \\'', new_data, '\\' does not exist.', sep='')\n",
    "            continue\n",
    "        else:\n",
    "            print('\\n')\n",
    "            yesno = str(input(f'Confirm that \\'{new_data}\\' is the correct '\n",
    "                              'filepath (\\'y\\' or \\'n\\'): '))\n",
    "            \n",
    "            if (yesno[0].lower() == \"y\"):\n",
    "                with open(new_data, encoding=\"utf8\") as json_file:\n",
    "                     json_obj = [json.loads(line) for line in json_file]\n",
    "                break\n",
    "            elif (yesno[0].lower() == 'n'):\n",
    "                print('\\n')\n",
    "                continue\n",
    "            else:\n",
    "                print('\\n')\n",
    "                print('Improper input')\n",
    "                continue\n",
    "    \n",
    "    #-----------------------------------------\n",
    "    # UNPACK RAW DATA\n",
    "    \n",
    "    json_obj2 = []\n",
    "    # append 'data' dictionary only\n",
    "    for x in range(0, len(json_obj)):\n",
    "        json_obj2.append(json_obj[x][\"data\"])\n",
    "    \n",
    "    # Check\n",
    "    if (len(json_obj2) - len(json_obj)) != 0:\n",
    "        print('*** ERROR: Did not extract all json \\'data\\' entries ***')\n",
    "\n",
    "    # ---- CONVERT TO DATAFRAME ----\n",
    "    df_raw_json = pd.DataFrame(json_obj2)\n",
    "    \n",
    "    # ---- UNPACK DICTIONARY ENTRIES ----\n",
    "    # 'category'\n",
    "    df_category = json_normalize(data=df_raw_json['category'])\n",
    "    df_category.columns = 'category_' + df_category.columns\n",
    "    \n",
    "    # 'creator'\n",
    "    df_creator = json_normalize(data=df_raw_json['creator'])\n",
    "    df_creator.columns = 'creator_' + df_creator.columns\n",
    "    \n",
    "    # 'location'\n",
    "    # Must mannually unpack 'location' with pd.Series due to NaN elements\n",
    "    df_location = df_raw_json['location'].apply(pd.Series)\n",
    "    df_location.drop(columns=0, inplace=True)\n",
    "    df_location.columns = 'location_'+df_location.columns\n",
    "    df_location1 = df_location['location_urls'].apply(pd.Series)\n",
    "    df_location1.drop(columns=0, inplace=True)\n",
    "    df_location1.columns = 'location_urls_'+df_location1.columns\n",
    "    df_location2 = df_location1['location_urls_web'].apply(pd.Series)\n",
    "    df_location2.drop(columns=0, inplace=True)\n",
    "    df_location2.columns = 'location_urls_web_'+df_location2.columns\n",
    "    df_location3 = df_location1['location_urls_api'].apply(pd.Series)\n",
    "    df_location3.drop(columns=0, inplace=True)\n",
    "    df_location3.columns = 'location_urls_api_'+df_location3.columns\n",
    "    # Concat 'location' dataframes\n",
    "    df_location = pd.concat([df_location, df_location2, df_location3], axis=1)\n",
    "    df_location.drop(columns='location_urls', inplace=True)\n",
    "    \n",
    "    # 'photo'\n",
    "    df_photo = json_normalize(data=df_raw_json['photo'])\n",
    "    df_photo.columns = 'photo_' + df_photo.columns\n",
    "    \n",
    "    # 'profile'\n",
    "    df_profile = json_normalize(data=df_raw_json['profile'])\n",
    "    df_profile.columns = 'profile_' + df_profile.columns\n",
    "    \n",
    "    # 'urls'\n",
    "    df_urls = json_normalize(data=df_raw_json['urls'])\n",
    "    df_urls.columns = 'urls_' + df_urls.columns\n",
    "     \n",
    "    # ---- CONCAT UNPACKED DATAFRAMES ----\n",
    "    df_raw = pd.concat([df_raw_json, df_category, df_creator, df_location, \n",
    "                        df_photo, df_profile, df_urls], axis=1)\n",
    "    df_raw.drop(columns=['category','creator','location','photo',\n",
    "                         'profile','urls'], inplace=True)\n",
    "    \n",
    "    #-----------------------------------------\n",
    "    # WRITE OUT\n",
    "    \n",
    "    df_raw.to_csv('data/df_raw.csv', sep=\",\")\n",
    "    \n",
    "    return df_raw\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='A1.2'></a>\n",
    "\n",
    "## A1.2 Data cleaning (as of 2018-12-12)\n",
    "\n",
    "**f_cleanData.py**\n",
    "\n",
    "```\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Dec 10 13:04:33 2018\n",
    "\n",
    "@author: steve\n",
    "\"\"\"\n",
    "\n",
    "#==============================================================================\n",
    "#\n",
    "# IMPORT LIBRARIES\n",
    "#\n",
    "#==============================================================================\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "#==============================================================================\n",
    "#\n",
    "# FUNCTION - CLEAN DATA\n",
    "#\n",
    "#==============================================================================\n",
    "def cleanData(df):\n",
    "    '''\n",
    "    This function cleans downloaded raw data for the Kickstarter success \n",
    "    prediction project. The input dataframe must be imported and the json \n",
    "    extracted using '00 - Data Import.py'. There should be either 95 or 96\n",
    "    columns (the 'state' column is optional) and they must be labeled exactly\n",
    "    as defined in '00 - Data Import.py'.\n",
    "    '''\n",
    "    \n",
    "    #-----------------------------------------\n",
    "    # ADD EMPTY STATE COLUMN IF NECESSARY\n",
    "    if 'state' not in df.columns:\n",
    "        df['state'] = None\n",
    "    \n",
    "    #-----------------------------------------\n",
    "    # COLUMN CLEANUP\n",
    "    \n",
    "    drop_vars = ['photo_1024x576', 'photo_1536x864', 'photo_ed', 'photo_full', \n",
    "                 'photo_key', 'photo_little', 'photo_med', 'photo_small', \n",
    "                 'photo_thumb', 'slug', 'urls_api.message_creator', 'urls_api.star', \n",
    "                 'urls_web.message_creator', 'urls_web.project', 'urls_web.rewards', \n",
    "                 'source_url', 'creator_avatar.medium', 'creator_avatar.small', \n",
    "                 'creator_avatar.thumb', 'creator_chosen_currency', 'creator_id', \n",
    "                 'creator_name', 'creator_slug', 'creator_urls.api.user',\n",
    "                 'creator_urls.web.user', 'location_id', 'location_name', \n",
    "                 'location_slug', 'location_short_name', 'location_displayable_name', \n",
    "                 'location_localized_name', 'location_type', 'location_is_root', \n",
    "                 'location_urls_web_discover', 'location_urls_web_location', \n",
    "                 'location_urls_api_nearby_projects', 'category_color', \n",
    "                 'category_id', 'category_urls.web.discover', \n",
    "                 'profile_background_color', \n",
    "                 'profile_background_image_attributes.id', \n",
    "                 'profile_background_image_attributes.image_urls.baseball_card', \n",
    "                 'profile_background_image_attributes.image_urls.default',\n",
    "                 'profile_background_image_opacity', 'profile_blurb', \n",
    "                 'profile_feature_image_attributes.id', \n",
    "                 'profile_feature_image_attributes.image_urls.baseball_card',\n",
    "                 'profile_feature_image_attributes.image_urls.default', 'profile_id',\n",
    "                 'profile_link_background_color', 'profile_link_text', \n",
    "                 'profile_link_text_color', 'profile_link_url', 'profile_name', \n",
    "                 'profile_project_id', 'profile_should_show_feature_image_section', \n",
    "                 'profile_show_feature_image', 'profile_state', \n",
    "                 'profile_state_changed_at', 'profile_text_color', 'currency_symbol',\n",
    "                 'static_usd_rate','converted_pledged_amount','fx_rate',\n",
    "                 'current_currency', 'usd_pledged', 'is_starrable', 'friends', \n",
    "                 'is_backing', 'is_starred', 'permissions', 'name', 'blurb',\n",
    "                 'location_state', 'location_country', 'currency', \n",
    "                 'currency_trailing_code', 'state_changed_at', 'category_parent_id', \n",
    "                 'category_position', 'category_name', 'category_id', \n",
    "                 'creator_is_registered', 'disable_communication', 'created_at', \n",
    "                 'usd_type']\n",
    "\n",
    "    df.drop(columns=drop_vars, inplace=True)\n",
    "    \n",
    "    # Rename columns\n",
    "    df.rename(columns={'category_slug':'category', 'state':'launch_state'}, inplace=True)\n",
    "    \n",
    "    # Rearrange columns\n",
    "    df = df[['launch_state', 'id', 'category', 'goal', 'backers_count', \n",
    "             'pledged', 'country','deadline', 'launched_at', \n",
    "             'staff_pick', 'spotlight']]\n",
    "\n",
    "    #-----------------------------------------\n",
    "    # EXTRACT CATEGORIES\n",
    "    df['category'] = [i.split('/')[0] for i in df['category']]\n",
    "    \n",
    "    #-----------------------------------------\n",
    "    # REMOVE DUPLICATES\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    # For duplicate IDs leftover, remove the lesser pledged row\n",
    "    df = df.sort_values('pledged', ascending=False).drop_duplicates('id').sort_index()\n",
    "    \n",
    "    # Check\n",
    "    if (len(df) - len(df[\"id\"])) != 0:\n",
    "        print('*** WARNING: There are ',\n",
    "              len(df) - len(df[\"id\"]), \n",
    "              ' duplicate IDs ***', sep='')\n",
    "    \n",
    "    #-----------------------------------------\n",
    "    # CONVERT DATETIMES\n",
    "    df['deadline'] = df['deadline'].apply(datetime.utcfromtimestamp)\n",
    "    df['launched_at'] = df['launched_at'].apply(datetime.utcfromtimestamp)\n",
    "    \n",
    "    #-----------------------------------------\n",
    "    # NA IMPUTATION \n",
    "    # Checks\n",
    "    if df.isnull().sum().sum() != 0:\n",
    "        print('*** WARNING: There are null values ***')\n",
    "    if df.isna().sum().sum() != 0:\n",
    "        print('*** WARNING: There are NA values ***')\n",
    "    if (df=='').sum().sum() != 0:\n",
    "        print('*** WARNING: There are empty string (\\'\\') values ***')\n",
    "    \n",
    "    #-----------------------------------------\n",
    "    # CLEAN UP 'launch_state'\n",
    "    df.query(\"launch_state == 'failed' | \"\n",
    "             \"launch_state == 'successful' | \"\n",
    "             \"launch_state == None\", inplace=True)\n",
    "    \n",
    "    #-----------------------------------------\n",
    "    # CONVERT CATEGORICAL VARIABLES TO DUMMY VARIABLES \n",
    "    category = pd.get_dummies(df['category'], drop_first=True)\n",
    "    country = pd.get_dummies(df['country'], drop_first=True)\n",
    "    d_launch_state = dict(zip(['failed','successful'], range(0,2)))\n",
    "    launch_state = df['launch_state'].map(d_launch_state)\n",
    "    \n",
    "    # Check\n",
    "    if (df[df['launch_state'] == 'successful'].shape[0] - launch_state.sum() != 0):\n",
    "        print('*** WARNING: Some launch_states did not map to 0/1 ***')\n",
    "    \n",
    "    # Drop the categorical launch_state column \n",
    "    # (keep 'category' and 'country' for  visualization)\n",
    "    df.drop(['launch_state'],axis=1,inplace=True)\n",
    "    \n",
    "    # Add the new dummy variable launch_state column and move it to column index 0 \n",
    "    # and country to column index 3\n",
    "    df = pd.concat([launch_state, df], axis=1)\n",
    "    df = df[['launch_state', 'id', 'category', 'country', 'goal', 'backers_count', \n",
    "             'pledged','deadline', 'launched_at', 'staff_pick', 'spotlight']]\n",
    "    \n",
    "    # Add the dummy variable country and category columns\n",
    "    df = pd.concat([df, category, country], axis=1)\n",
    "    \n",
    "    # Checks\n",
    "    if (df.isnull().sum().sum() != 0):\n",
    "        print('*** WARNING: Null values introduced with dummy variables ***')\n",
    "    if (df.isna().sum().sum() != 0):\n",
    "        print('*** WARNING: NA values introduced with dummy variables ***')\n",
    "    if (df=='').sum().sum() != 0:\n",
    "        print('*** WARNING: Empty string (\\'\\') values introduced with dummy variables ***')\n",
    "    \n",
    "    #-----------------------------------------\n",
    "    # FINAL CLEANUP\n",
    "    # pledged_ratio\n",
    "    pledged_ratio = df['pledged'] / df['goal']\n",
    "    df.insert(loc=df.columns.get_loc(\"pledged\"), column='pledged_ratio', \n",
    "              value=pledged_ratio)\n",
    "    df.drop(columns='pledged', inplace=True)\n",
    "    \n",
    "    # datetime columns\n",
    "    funding_days = (df['deadline'] - df['launched_at']).dt.days\n",
    "    df.insert(loc=df.columns.get_loc(\"deadline\"), column='funding_days', \n",
    "              value=funding_days)\n",
    "    df.drop(columns='deadline', inplace=True)\n",
    "    \n",
    "    # ---- MOVE 'LAUNCHED_AT' ----\n",
    "    launched_at = df['launched_at']\n",
    "    df.drop(columns='launched_at', inplace=True)\n",
    "    df.insert(loc=2, column='launched_at', value=launched_at)\n",
    "    \n",
    "    # ---- CONVERT 'STAFF_PICK' AND 'SPOTLIGHT' TO DUMMIES ----\n",
    "    d_staff_pick = dict(zip([False,True], range(0,2)))\n",
    "    staff_pick = df['staff_pick'].map(d_staff_pick)\n",
    "    \n",
    "    # Check\n",
    "    if (df[df['staff_pick'] == True].shape[0] - staff_pick.sum()) != 0:\n",
    "        print('*** WARNING: \\'staff_pick\\' not mapped to 0/1 properly ***')\n",
    "        \n",
    "    d_spotlight = dict(zip([False,True], range(0,2)))\n",
    "    spotlight = df['spotlight'].map(d_spotlight)\n",
    "    \n",
    "    # Check\n",
    "    if (df[df['spotlight'] == True].shape[0] - spotlight.sum()) != 0:\n",
    "        print('*** WARNING: \\'spotlight\\' not mapped to 0/1 properly ***')\n",
    "        \n",
    "    df.drop(['staff_pick','spotlight'],axis=1,inplace=True)\n",
    "    \n",
    "    df.insert(loc=df.columns.get_loc(\"comics\"), column='staff_pick', value=staff_pick)\n",
    "    df.insert(loc=df.columns.get_loc(\"comics\"), column='spotlight', value=spotlight)\n",
    "    \n",
    "    #-----------------------------------------\n",
    "    # VARIABLE REDUCTION\n",
    "    df.drop(columns='spotlight', inplace=True)\n",
    "    \n",
    "    # ---- NULL/NA/EMPTY CHECKS ----\n",
    "    if (df.isnull().sum().sum() != 0):\n",
    "        print('*** WARNING: Null values introduced with \\'staff_pick\\' and \\'spotlight\\' dummy variables ***')\n",
    "    if (df.isna().sum().sum() != 0):\n",
    "        print('*** WARNING: NA values introduced with \\'staff_pick\\' and \\'spotlight\\' dummy variables ***')\n",
    "    if (df=='').sum().sum() != 0:\n",
    "        print('*** WARNING: Empty string (\\'\\') values introduced with \\'staff_pick\\' and \\'spotlight\\' dummy variables ***')\n",
    "    \n",
    "    #-----------------------------------------\n",
    "    # WRITE OUT\n",
    "    df.to_csv('df_clean.csv', sep=\",\")\n",
    "    \n",
    "    return df\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
